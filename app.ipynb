{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from groq import Groq"
      ],
      "metadata": {
        "id": "2t2f0yCpa4aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.title(\"ðŸ¦œðŸ”— QA Chatbot Application\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Choose a PDF file\", type=[\"pdf\"])"
      ],
      "metadata": {
        "id": "_MZJmge9a7ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "Bll966pLa_Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHROMA_PATH = \"./chroma_db\""
      ],
      "metadata": {
        "id": "8O0ACJ7ybAdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(api_key=GROQ_API_KEY)"
      ],
      "metadata": {
        "id": "vbq3iJYxbB-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU5vPyVOawj8"
      },
      "outputs": [],
      "source": [
        "if uploaded_file is not None:\n",
        "    file_path = os.path.join(\"data\", uploaded_file.name)\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(uploaded_file.getbuffer())\n",
        "\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "    db = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=CHROMA_PATH)\n",
        "    retriever = db.as_retriever()\n",
        "\n",
        "    st.success(\"PDF processed and vector store created!\")\n",
        "\n",
        "    def generate_response(query):\n",
        "        docs = retriever.get_relevant_documents(query)\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs[:3]])\n",
        "\n",
        "        full_prompt = f\"Answer the question based on the following context, answer the questions in a very detailed manner and use markdown for formatting:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            model=\"llama-3-70b-8192\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "\n",
        "    with st.form(\"query_form\"):\n",
        "        user_query = st.text_area(\"Ask a question about the PDF:\")\n",
        "        submitted = st.form_submit_button(\"Submit\")\n",
        "        if submitted:\n",
        "            with st.spinner(\"Generating response...\"):\n",
        "                response = generate_response(user_query)\n",
        "                st.info(response)"
      ]
    }
  ]
}